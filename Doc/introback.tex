\begin{abstract}

Inferred invariants can show specification of systems, act as documentation and help program comprehension. This is even more useful in a distributed system where the state is defined globally across nodes and errors are propogated across node boundaries. But defining invariants manually is a laborious task and not feasible for a typical software system. So we propose a technique for inferring likely invariants of distributed systems from run time execution. We use data flow and control flow analyses to identify what contributes to the distributed state of the system. Then we use executions of the program to get runtime values and then use Daikon to infer invariants between variables. To achieve this, we have implemented a program slicer for Go programs, an instrumentation framework and a program--trace analyzer in Go. For evaluating our tool, we have successfully inferred invariants in few distributed programs including the two-phase-commit algorithm.
\end{abstract}
\section{Introduction}

Building a reliable distributed system is a difficult task, since distributed system should continue operating even when components or the network fail, this means that there are many corner cases that developers should consider when building a system. Building such 
a reliable and dependable system is not possible without thorough testing. However, testing a distributed system is a difficult task because it is hard to test components of a distributed system in isolation. There are many 
failure scenarios that a distributed system should handle and these scenarios should be tested as well. In some cases reproducing the failures is a difficult task e.g., degrading network performance for test purposes. Many bugs turn out to be heisenbugs in a distributed 
system which are hard to reproduce and debug. Debugging and testing distributed systems is hard because a fault in a single node is not confined to that node, instead, it may cause an error in another node. In fact errors in distributed system should be defined on global state of the system and not on a single node. This also makes program comprehension of distributed systems a difficult task, since the developer needs to comprehend interaction of different nodes as well as the logic of each node.

To test and debug distributed systems, developers and testers have to spend a considerable amount of time inspecting logs of the system manually and look for incidents the system does not behave as 
expected. One alternative to this laborious approach is to define invariants for a system's expected behaviour and check the software traces against these invariants. This approach is considerably easier than the former but as the system gets larger and more 
complex, providing invariants manually becomes more difficult. In this work, we automate the process of defining invariants by inferring likely invariants of distributed systems from run time execution. We define an invariant of distributed systems as a property that holds at a certain consistent cut across multiple nodes in distributed state of a distributed system. These inferred invariants can ease many programming tasks, such as program comprehension, bug prevention, debugging, verification and testing. Inferred invariants can show specification of systems,  act as documentation and help program comprehension. They can be checked after each modification to source code to make sure that invariants of the system that implies its correctness are not violated, thus helping bug prevention. Even if the bugs get into the system, these invariants can help debugging by locating the points in program that they got violated. Moreover, they can be written in form of assertions and be used for automated test generation. 
%Ernst et al \cite{ernst2007daikon} define a program invariant as ``a property that holds at a certain point or points in a program". 





%The most challenging part of this work is to find valid snapshots of distributed states for the system. 
Consider pseudo-two-phase-commit protocol in figure \ref{lst:2pc}. In this protocol the coordinator first queries other nodes for their vote and if all nodes including the coordinator vote ``Commit" then the coordinator sends ``Commit", otherwise it sends ``Abort" to all other nodes. At the end of this protocol all nodes should either commit or abort . To verify that this algorithm is correct, developers can examine inferred invariants of the protocol execution and inspect if these inferred invariants match expected behaviour of the system (In this case commit value of all nodes should have the same value i.e., $coordinator.commit$ = $replica_i .commit$ for each replica).


\begin{figure}
\centering
\begin{minipage}{.45\textwidth}
\begin{lstlisting}[caption={Coordinator Code}]
for i := 1 to n do
	send('Query',i)
commit := decide()
for i:= 1 to n do
	recv(buf,i)
	if (buf == 'Abort')
		commit := 'Abort'
for i := 1 to n
	send(commit, i)
\end{lstlisting}
\end{minipage}\hfill
\begin{minipage}{.45\textwidth}
\begin{lstlisting}[caption={Replica Code}]
recv(buf)
if(buf == 'Query')
	vote:= decide()
	send(vote)
if (vote == 'Abort')
	commit = 'Abort'
else
	recv(buf)
	commit = buf
\end{lstlisting}
\end{minipage}\hfill
\caption{Pseudo two phase commit code for coordinator and replicas}
\label{lst:2pc}
\end{figure}


%\begin{figure}
%\centering
%\begin{lstlisting}[caption={Coordinator Code}]
%for i := 1 to n do
%	send('Query',i)
%commit := decide()
%for i:= 1 to n do
%	recv(buf,i)
%	if (buf == 'Abort')
%		commit := 'Abort'
%for i := 1 to n
%	send(commit, i)
%\end{lstlisting}
%\begin{lstlisting}[caption={Replica Code}]
%recv(buf)
%if(buf == 'Query')
%	vote:= decide()
%	send(vote)
%if (vote == 'Abort')
%	commit = 'Abort'
%else
%	recv(buf)
%	commit = buf
%\end{lstlisting}
%\caption{Pseudo two phase commit code for coordinator and replicas}
%\label{lst:2pc}
%\end{figure}

In this work we 
assume that state of each node is comprised of its variables. Not all variables of one node is affected by other nodes, we define the set of variables of one node whose values are affected by other nodes as distributed state variables of that node. We refer to the set of distributed 
state variables of all nodes as distributed state of the system. We use Daikon \cite{ernst2001dynamically} to infer invariants between variables, so we can infer linear ($y = ax + b$) and ordering ($y < x$) relationships that may hold between the distributed state variables of all nodes in the system.



\section{Background}

Invariants are properties that should hold in certain points in a program. Although previous works \cite{ernst2001dynamically} showed that invariant inference is feasible in sequential programs, inferring invariants in a distributed system poses unique challenges and still is an open problem.To infer invariants in a distributed system we should have global snapshots (log of variables) of the system. However, not every snapshot of the system is a valid one. Mandy and Lamport introduced the notion of cut and consistent cut as snapshot and valid snapshots of a system. In \cite{mattern1989virtual}, Mattern proposed an algorithm to find consistent cuts. We used vector clocks \cite{vectorclock} to partially order snapshots of the nodes in our system and form consistent cuts. The notion of distributed state first proposed by Ousterhout in \cite{ousterhout1991role} as ``information retained in one place that describes something, or is determined by something, somewhere else in the system", we assume that variables whose value is affected by other nodes constitute a significant portion of distributed state of that node. We use static data flow and control flow analysis to identify the distributed state of the system. To do so we computed the forward and backward slices of the program  \cite{programslice} from communication points in the program, such as receive and send functions, and determined list of variables whose values are affected by a receive function or can affect values that are sent by a send function. By doing so, we identified the variables that contribute to interactions between nodes, possibly affect values of variables in other nodes, hence form the distributed state of the program.

We can view our approach as an optimization. The naive approach to solve inference problem is to log all of the variables at each instruction execution and give all consistent cuts of the system to a invariant detector tool to infer invariants between the variables. However, this naive approach is not efficient at all, if a total number of $n$ instruction is executed during a run by all nodes in the system and on average we have $m$ variables for each node, we should log $n*m$ variables but many of these variables are local to one node and are not affected by other nodes, so we may even end up finding many false positives (invariants that hold by accident and are not meaningful). Our approach only logs variables that an invariant might exist between them by using data and control flow analysis and finding group of variables (across the nodes) which affect each other. 
Another optimization that our approach does is that instead of logging the variables at each instruction, we only log them at consistent cuts annotated by developers. Based on our observation some must-hold invariants can be violated for small amount of time (e.g., execution time of several instructions) until systems goes to a stable state, so we decided to give developers responsibility to decide at which points in program invariants should hold and can be checked, by annotating the code. By this approach we improve usefulness of our tool for developers. Another approach can be inferring the invariants from logs of the system, our approach removes the burden from developers to decide which portion of each node's state needs to be logged. We automatically infer the variables that are likely to have invariants between them with data and control flow analysis.