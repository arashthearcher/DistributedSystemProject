\section{Introduction}

Building a reliable distributed system is a difficult task, since distributed system should continue operating even when components or network fail, this means that there are many corner cases that developers should consider when building the system. Building such 
a reliable and dependable system is not possible without thorough testing of the system. However, testing a distributed system is a much more difficult task because it is much harder to test components of a distributed system in isolation. There are many 
failure scenarios that distributed system should handle and these scenarios should be tested as well. In some cases reproducing the failures is a difficult task e.g. degrading network performance for test purposes. Many bugs turn out to be heisenbugs in distributed 
system which are hard to reproduce and detect. For testing and debugging distributed systems, developers and testers have to spend a considerable amount of time inspecting logs of the system manually and looking for incidents that system does not behave as 
expected. One alternative to this laborious approach is to define invariants for system's expected behaviour and check the software traces against these invariants. This approach is considerably easier than the former but as the system gets larger and more 
complex, providing invariants manually becomes more difficult.

Testing distributed systems is hard because a fault in a single node does not confine to that node, instead, it may cause an error in another node. In fact errors in distributed system should be defined on global state of the system and not on a single node. In this work we 
assume that state of each node is comprised of its variables. Not all variables of one node is affected by other nodes, we define the set of variables of one node whose values are affected by other nodes as distributed state variables of that node. We refer to set of distributed 
state variables of all nodes as distributed state of the system.

%The most challenging part of this work is to find valid snapshots of distributed states for the system. 

\section{Motivating Example}
For motivation example, consider pseudo-two-phase-commit protocol of figure \ref{lst:2pc}. In this protocol coordinator first queries other nodes for their vote and if all nodes including the coordinator vote ``Commit" then coordinator sends ``Commit", otherwise it sends ``Abort" to all other nodes. At the end of this protocol all nodes should either commit or abort . In order to verify that this algorithm is working correctly, developers can examine inferred invariants of the protocol execution and inspect if these inferred invariants match expected behaviour of the system (In this case commit value of all nodes should have the same value and this property should be inferred by executing the protocol). We use daikon \cite{ernst2001dynamically} to infer invariants between variables so we can infer linear ($y = ax + b$), ordering ($y < x$) and containment ($x \in y$) relationships that may hold between the distributed state variables of all nodes in the system.


\begin{figure}
\centering
\begin{lstlisting}[caption={Coordinator Code}]
for i := 1 to n do
	send('Query',i)
commit := decide()
for i:= 1 to n do
	recv(buf,i)
	if (buf == 'Abort')
		commit := 'Abort'
for i := 1 to n
	send(commit, i)
\end{lstlisting}
\begin{lstlisting}[caption={Coordinator Code}]
recv(buf)
if(buf == 'Query')
	vote:= decide()
	send(vote)
if (vote == 'Abort')
	commit = 'Abort'
else
	recv(buf)
	commit = buf
\end{lstlisting}
\caption{Pseudo two phase commit code for coordinator and cohorts}
\label{lst:2pc}
\end{figure}

\section{Background}

Invariants are properties that should hold in  certain points in program. Although previous works \cite{ernst2001dynamically} showed that invariant inference is feasible in sequential programs, inferring invariants in a distributed system poses unique challenges and still is a open problem. To infer invariants in a distributed system we should have global snapshots (log of variables) of the system. However, not every snapshot of the system is a valid one. Mattern introduced the notion of cut and consistent cut as snapshot and valid snapshots of a system. In \cite{mattern1989virtual} he also proposed an algorithm to find consistent cuts. The notion of distributed state first proposed by Ousterhout in \cite{ousterhout1991role} as ``information retained in one place that describes something, or is determined by something, somewhere else in the system", we assume that variables whose value is affected by other nodes consists a significant portion of distributed state of that node. We use dynamic data flow analysis to detect the distributed state of the system. We also use vector clocks to partially order snapshots of the nodes in our system and form consistent cuts.

Our approach is to automatically infer invariants that may exist in distributed state of our system. To this end, we execute distributed programs by running the test cases and dynamically select a set of variables in each node that likely represent the distributed state 
across our distributed nodes. We do so with dynamic data flow analysis of variables and selecting those that are affected by a send or receive operation. Finally we use Daikon to infer invariants among our distributed state variables. \\ \\
We can view our approach as an optimization. The naive approach to solve inference problem is to log all of the variables at each instruction execution and give all consistent cuts of the system to daikon to infer invariants between the variables. However, this naive approach is not efficient at all, if a total number of n instruction is executed during a run by all nodes in the system and on average we have m variables for each node, we should log $n*m$ variables but many of these variables are local to one node and are not affected by other nodes, so we may even end up finding many false positives (invariants that hold by accident and are not meaningful). Our approach only logs variables that an invariant might exist between them by using data flow analysis and finding group of variables (across the nodes) which affect each other. 
Another optimization that our approach does is that instead of logging the variables at each instruction, we only log them at consistent cuts annotated by developers. Based on our observation some must-hold invariants can be violated for small amount of time (e.g. execution time of several instructions) until systems goes to a stable state, so we decided to give developers responsibility to decide at which points in program invariants should hold and can be checked by annotating the code. By this approach we improve usefulness of our tool for developers. Another approach can be inferring the invariants from logs of the system, our approach removes the burden from developers to decide which portion of each node's state needs to be logged. We automatically infer variables that are likely to have invariants between them with data flow analysis.